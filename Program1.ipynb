{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0850665d-debc-4cc0-90ae-91ce5742a444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing\n",
    "# stop word removal, word tokenization, sentence tokenization, uppercase lowercase text, \n",
    "# pattern matching (date email etc), dirty text cleaning (removal of special characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9caae810-a1a3-4725-8167-9ac931e140b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting numpy\n",
      "  Using cached numpy-1.26.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
      "Collecting nltk\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.4.0-1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
      "Collecting tzdata>=2022.7\n",
      "  Using cached tzdata-2023.4-py2.py3-none-any.whl (346 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas) (2022.1)\n",
      "Collecting regex>=2021.8.3\n",
      "\u001b[33m  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7f86d18ad420>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /packages/81/8a/96a62ce98e8ff1b16db56fde3debc8a571f6b7ea42ee137eb0d995cdfa26/regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\u001b[0m\u001b[33m\n",
      "\u001b[0m  Downloading regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m774.0/774.0 KB\u001b[0m \u001b[31m612.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tqdm\n",
      "  Using cached tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "Collecting joblib\n",
      "  Using cached joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk) (8.0.3)\n",
      "Collecting scipy>=1.6.0\n",
      "  Downloading scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.4 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.4/38.4 MB\u001b[0m \u001b[31m496.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:02\u001b[0m\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
      "  Using cached threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Installing collected packages: tzdata, tqdm, threadpoolctl, regex, numpy, joblib, scipy, pandas, nltk, scikit-learn\n",
      "\u001b[33m  WARNING: The script tqdm is installed in '/home/student/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script f2py is installed in '/home/student/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script nltk is installed in '/home/student/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed joblib-1.3.2 nltk-3.8.1 numpy-1.26.3 pandas-2.2.0 regex-2023.12.25 scikit-learn-1.4.0 scipy-1.12.0 threadpoolctl-3.2.0 tqdm-4.66.1 tzdata-2023.4\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy pandas scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f406778e-690c-4321-9b48-3e6d4cccb566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\91720\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (3.8.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\91720\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from nltk) (2023.8.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\91720\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\91720\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: click in c:\\users\\91720\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\91720\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from click->nltk) (0.4.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\91720\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install --user -U nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92309d4-da12-496c-8a08-a7e472d269ca",
   "metadata": {},
   "source": [
    "***Word tokenization***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44fef0cc-6da6-4976-8063-4459f94dd1a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "# word_tokens = word_tokenize(text)\n",
    "\n",
    "filtered_sentence = []\n",
    "for word in text.split(' '):\n",
    "    filtered_sentence.append(word)\n",
    "\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d77b492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "filtered = word_tokenize(text)\n",
    "print(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925037f2-a61a-4bf2-8f60-2f11b2f59445",
   "metadata": {},
   "source": [
    "***Sentence tokenization***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c7a67b4-e934-46d0-b069-037310e25d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is a simple sentence to check the sentence tokenization', ' Zeeshan sir told us to do it for our NLP lab', ' This system runs on Ubuntu', '']\n"
     ]
    }
   ],
   "source": [
    "text2 = \"This is a simple sentence to check the sentence tokenization. Zeeshan sir told us to do it for our NLP lab. This system runs on Ubuntu.\"\n",
    "\n",
    "sentence_tokens = []\n",
    "\n",
    "for word in text2.split('.'):\n",
    "    sentence_tokens.append(word)\n",
    "\n",
    "print(sentence_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce3219a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is a simple sentence to check the sentence tokenization.', 'Zeeshan sir told us to do it for our NLP lab.', 'This system runs on Ubuntu.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "sentences = sent_tokenize(text2)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64eb6a20-8fdd-44ee-8f67-b3464e881898",
   "metadata": {},
   "source": [
    "***Letter uppercase lowercase conversion***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1db39dab-1673-4e1c-bf14-71f263460534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THIS IS A SIMPLE TEXT TO SHOW THAT 56 > 34\n",
      "this is a simple text to show that 56 > 34\n"
     ]
    }
   ],
   "source": [
    "text3 = \"this is a simple text to show that 56 > 34\"\n",
    "\n",
    "text3 = text3.upper()\n",
    "print(text3)\n",
    "\n",
    "text3 = text3.lower()\n",
    "print(text3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e06784-808c-4c8f-b0b5-387b614097f0",
   "metadata": {},
   "source": [
    "***Stop word removal***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b02e1f3-3535-48f2-8514-918ded374f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'quick', 'brown', 'fox', 'jumps', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "text = \"The quick brown fox jumps over the lazy dog\"\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "stopwords_removed = []\n",
    "\n",
    "for word in filtered_sentence:\n",
    "    if word not in stop_words:\n",
    "        stopwords_removed.append(word)\n",
    "\n",
    "print(stopwords_removed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb371d6f-3751-47b0-a6c1-5021761bc7a3",
   "metadata": {},
   "source": [
    "***Stemming and Lemmatization***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0637759e-0e2c-4cca-bbe7-93dcc2b32cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walking (after stemming): walk\n",
      "sung (after lemmatization): sing\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('punkt')\n",
    "ps = PorterStemmer()\n",
    "wl = WordNetLemmatizer()\n",
    "\n",
    "text5 = \"walking\"\n",
    "text6 = \"sung\"\n",
    "\n",
    "print(text5 + \" (after stemming): \" + ps.stem(text5))\n",
    "print(text6 + \" (after lemmatization): \"  + wl.lemmatize(text6, pos='v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bae010-5c41-4016-a492-600b988e294b",
   "metadata": {},
   "source": [
    "***Remove punctuation and special characters***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08960aaf-19fa-42a4-8ed2-844f18ac1de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is text to show special maybe they are special i dont know exactly why they are called special characters     \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text4 = \"This is text to show special (maybe, they are special; i don't know exactly why they are called special) characters {} [] | + $$$.\"\n",
    "pattern = \"[^A-Za-z0-9 ]+\"\n",
    "\n",
    "res = re.sub(pattern, '', text4)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d974638f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(,;'){}[]|+$$$.\n"
     ]
    }
   ],
   "source": [
    "pattern = '[^a-zA-Z0-9 ]+'\n",
    "res = re.sub(pattern, '', text4)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d27517-97d2-43b6-922f-9af0adb89ffd",
   "metadata": {},
   "source": [
    "***Remove URLs and HTML tags***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "849f54ce-99c7-4481-9138-7714b7a1028b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sample text with the URL https://www.google.com\n",
      "This is a sample text with the URL \n"
     ]
    }
   ],
   "source": [
    "text5 = \"\"\"<html>This is a sample text with the URL https://www.google.com<a href=\"https://some-url-here\"></a></html>\"\"\"\n",
    "p = re.compile(r'<.*?>')\n",
    "ans = p.sub('', text5)\n",
    "print(ans)\n",
    "ans = re.sub(r'https?://\\S+', '', ans)\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "648f04f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URLs: ['https://www.google.com', 'https://some-url-here', 'http://example.com']\n",
      "Text without HTML tags and URLs: This is a sample text with the URL  and  overlapping.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_urls_and_text(html_text):\n",
    "    # Pattern to extract URLs\n",
    "    url_pattern = r'(?:https?://|www\\.)\\S+?(?=[\\'\"\\s<]|$)'\n",
    "\n",
    "    # Pattern to extract text within HTML tags\n",
    "    html_pattern = r'<[^>]*>'\n",
    "\n",
    "    # Extract URLs\n",
    "    urls = re.findall(url_pattern, html_text)\n",
    "\n",
    "    # Remove URLs from the text\n",
    "    text_without_urls = re.sub(url_pattern, '', html_text)\n",
    "\n",
    "    # Remove HTML tags to get clean text\n",
    "    text_without_html = re.sub(html_pattern, '', text_without_urls)\n",
    "\n",
    "    return urls, text_without_html\n",
    "\n",
    "text5 = \"\"\"<html>This is a sample text with the URL https://www.google.com<a href=\"https://some-url-here\"></a> and http://example.com overlapping.</html>\"\"\"\n",
    "\n",
    "urls, text_without_html = extract_urls_and_text(text5)\n",
    "print(\"URLs:\", urls)\n",
    "print(\"Text without HTML tags and URLs:\", text_without_html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075495ff-5a3c-4085-a02f-a946fc02caa2",
   "metadata": {},
   "source": [
    "***Replace contractions with their explanations (ex: can't -> cannot)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb11c3d0-2c36-4df8-95d6-0c735f74f060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re.compile(\"\\\\b(?:can't|couldn't|didn't|doesn't|don't|hadn't|hasn't|haven't|isn't|shouldn't|wasn't|weren't|won't|wouldn't)\\\\b\")\n",
      "I cannot believe he did not come. She should not have done that.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def expand_apostrophe_words(text):\n",
    "    # Define a dictionary of apostrophe words and their full forms\n",
    "    apostrophe_words = {\n",
    "        \"can't\": \"cannot\",\n",
    "        \"couldn't\": \"could not\",\n",
    "        \"didn't\": \"did not\",\n",
    "        \"doesn't\": \"does not\",\n",
    "        \"don't\": \"do not\",\n",
    "        \"hadn't\": \"had not\",\n",
    "        \"hasn't\": \"has not\",\n",
    "        \"haven't\": \"have not\",\n",
    "        \"isn't\": \"is not\",\n",
    "        \"shouldn't\": \"should not\",\n",
    "        \"wasn't\": \"was not\",\n",
    "        \"weren't\": \"were not\",\n",
    "        \"won't\": \"will not\",\n",
    "        \"wouldn't\": \"would not\"\n",
    "        # Add more apostrophe words and their full forms as needed\n",
    "    }\n",
    "\n",
    "    # Create a regular expression pattern to match apostrophe words\n",
    "    pattern = re.compile(r'\\b(?:' + '|'.join(re.escape(word) for word in apostrophe_words.keys()) + r')\\b')\n",
    "    print(pattern)\n",
    "    # Replace apostrophe words with their full forms\n",
    "    expanded_text = pattern.sub(lambda match: apostrophe_words[match.group(0)], text)\n",
    "\n",
    "    return expanded_text\n",
    "\n",
    "# Example text containing apostrophe words\n",
    "text = \"I can't believe he didn't come. She shouldn't have done that.\"\n",
    "\n",
    "# Expand apostrophe words\n",
    "expanded_text = expand_apostrophe_words(text)\n",
    "print(expanded_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7717a892-93e5-4147-9d9c-0d5979a4a0fc",
   "metadata": {},
   "source": [
    "***Convert date and currency into single standard format***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1bce44ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting babel\n",
      "  Downloading Babel-2.15.0-py3-none-any.whl (9.6 MB)\n",
      "     ---------------------------------------- 9.6/9.6 MB 6.0 MB/s eta 0:00:00\n",
      "Installing collected packages: babel\n",
      "Successfully installed babel-2.15.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\91720\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install babel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8d61b299-4949-47e3-80e9-1b6dddc97c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Date: 27th May 2024\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "# from babel.numbers import parse_decimal, format_currency\n",
    "\n",
    "def convert_to_standard_date(date_str):\n",
    "    # List of possible date formats\n",
    "    date_formats = ['%Y-%m-%d', '%d-%m-%Y', '%m-%d-%Y', '%d/%m/%Y', '%m/%d/%Y']\n",
    "\n",
    "    # Try parsing the date with each format until successful\n",
    "    for date_format in date_formats:\n",
    "        try:\n",
    "            # Parse the date string\n",
    "            date_obj = datetime.strptime(date_str, date_format)\n",
    "            # Get the day\n",
    "            day = date_obj.strftime(\"%d\")\n",
    "            # Add suffix\n",
    "            if day.endswith('1') and day != '11':\n",
    "                suffix = 'st'\n",
    "            elif day.endswith('2') and day != '12':\n",
    "                suffix = 'nd'\n",
    "            elif day.endswith('3') and day != '13':\n",
    "                suffix = 'rd'\n",
    "            else:\n",
    "                suffix = 'th'\n",
    "            # Format the date as \"5th May 2024\"\n",
    "            formatted_date = date_obj.strftime(f\"%d{suffix} %B %Y\")\n",
    "            return formatted_date\n",
    "        except ValueError:\n",
    "            pass\n",
    "    \n",
    "    # If none of the formats match, return None\n",
    "    return None\n",
    "\n",
    "# def convert_to_standard_currency(currency_str, currency_code):\n",
    "#     try:\n",
    "#         # Parse the currency string\n",
    "#         value = parse_decimal(currency_str, locale='en')\n",
    "#         # Format the currency to the standard format\n",
    "#         return format_currency(value, currency_code, locale='en')\n",
    "#     except ValueError:\n",
    "#         # If parsing fails, return None\n",
    "#         return None\n",
    "\n",
    "date_str = \"2024-05-27\"\n",
    "\n",
    "standard_date = convert_to_standard_date(date_str)\n",
    "\n",
    "print(\"Standard Date:\", standard_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24b8397",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
